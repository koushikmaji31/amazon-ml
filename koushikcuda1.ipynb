{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/ai23mtech11004/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the allowed units (as provided)\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre', 'cubic foot', 'cubic inch', 'cup', 'decilitre', 'fluid ounce', 'gallon', 'imperial gallon', 'litre', 'microlitre', 'millilitre', 'pint', 'quart'}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test CSV file\n",
    "test_df = pd.read_csv('dataset/final_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.19it/s]\n",
      "/raid/ai23mtech11004/.local/lib/python3.8/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 10000\n",
      "Processed row 10001\n",
      "Processed row 10002\n",
      "Processed row 10003\n",
      "Processed row 10004\n",
      "Processed row 10005\n",
      "Processed row 10006\n",
      "Processed row 10007\n",
      "Processed row 10008\n",
      "Processed row 10009\n",
      "Processed row 10010\n",
      "Processed row 10011\n",
      "Processed row 10012\n",
      "Processed row 10013\n",
      "Processed row 10014\n",
      "Processed row 10015\n",
      "Processed row 10016\n",
      "Processed row 10017\n",
      "Processed row 10018\n",
      "Processed row 10019\n",
      "Processed row 10020\n",
      "Processed row 10021\n",
      "Processed row 10022\n",
      "Processed row 10023\n",
      "Processed row 10024\n",
      "Processed row 10025\n",
      "Processed row 10026\n",
      "Processed row 10027\n",
      "Processed row 10028\n",
      "Processed row 10029\n",
      "Processed row 10030\n",
      "Processed row 10031\n",
      "Processed row 10032\n",
      "Processed row 10033\n",
      "Processed row 10034\n",
      "Processed row 10035\n",
      "Processed row 10036\n",
      "Processed row 10037\n",
      "Processed row 10038\n",
      "Processed row 10039\n",
      "Processed row 10040\n",
      "Processed row 10041\n",
      "Processed row 10042\n",
      "Processed row 10043\n",
      "Processed row 10044\n",
      "Processed row 10045\n",
      "Processed row 10046\n",
      "Processed row 10047\n",
      "Processed row 10048\n",
      "Processed row 10049\n",
      "Processed row 10050\n",
      "Processed row 10051\n",
      "Processed row 10052\n",
      "Processed row 10053\n",
      "Processed row 10054\n",
      "Processed row 10055\n",
      "Processed row 10056\n",
      "Processed row 10057\n",
      "Processed row 10058\n",
      "Processed row 10059\n",
      "Processed row 10060\n",
      "Processed row 10061\n",
      "Processed row 10062\n",
      "Processed row 10063\n",
      "Processed row 10064\n",
      "Processed row 10065\n",
      "Processed row 10066\n",
      "Processed row 10067\n",
      "Processed row 10068\n",
      "Processed row 10069\n",
      "Processed row 10070\n",
      "Processed row 10071\n",
      "Processed row 10072\n",
      "Processed row 10073\n",
      "Processed row 10074\n",
      "Processed row 10075\n",
      "Processed row 10076\n",
      "Processed row 10077\n",
      "Processed row 10078\n",
      "Processed row 10079\n",
      "Processed row 10080\n",
      "Processed row 10081\n",
      "Processed row 10082\n",
      "Processed row 10083\n",
      "Processed row 10084\n",
      "Processed row 10085\n",
      "Processed row 10086\n",
      "Processed row 10087\n",
      "Processed row 10088\n",
      "Processed row 10089\n",
      "Processed row 10090\n",
      "Processed row 10091\n",
      "Processed row 10092\n",
      "Processed row 10093\n",
      "Processed row 10094\n",
      "Processed row 10095\n",
      "Processed row 10096\n",
      "Processed row 10097\n",
      "Processed row 10098\n",
      "Processed row 10099\n",
      "Processed row 10100\n",
      "Processed row 10101\n",
      "Processed row 10102\n",
      "Processed row 10103\n",
      "Processed row 10104\n",
      "Processed row 10105\n",
      "Processed row 10106\n",
      "Processed row 10107\n",
      "Processed row 10108\n",
      "Processed row 10109\n",
      "Processed row 10110\n",
      "Processed row 10111\n",
      "Processed row 10112\n",
      "Processed row 10113\n",
      "Processed row 10114\n",
      "Processed row 10115\n",
      "Processed row 10116\n",
      "Processed row 10117\n",
      "Processed row 10118\n",
      "Processed row 10119\n",
      "Processed row 10120\n",
      "Processed row 10121\n",
      "Processed row 10122\n",
      "Processed row 10123\n",
      "Processed row 10124\n",
      "Processed row 10125\n",
      "Processed row 10126\n",
      "Processed row 10127\n",
      "Processed row 10128\n",
      "Processed row 10129\n",
      "Processed row 10130\n",
      "Processed row 10131\n",
      "Processed row 10132\n",
      "Processed row 10133\n",
      "Processed row 10134\n",
      "Processed row 10135\n",
      "Processed row 10136\n",
      "Processed row 10137\n",
      "Processed row 10138\n",
      "Processed row 10139\n",
      "Processed row 10140\n",
      "Processed row 10141\n",
      "Processed row 10142\n",
      "Processed row 10143\n",
      "Processed row 10144\n",
      "Processed row 10145\n",
      "Processed row 10146\n",
      "Processed row 10147\n",
      "Processed row 10148\n",
      "Processed row 10149\n",
      "Processed row 10150\n",
      "Processed row 10151\n",
      "Processed row 10152\n",
      "Processed row 10153\n",
      "Processed row 10154\n",
      "Processed row 10155\n",
      "Processed row 10156\n",
      "Processed row 10157\n",
      "Processed row 10158\n",
      "Processed row 10159\n",
      "Processed row 10160\n",
      "Processed row 10161\n",
      "Processed row 10162\n",
      "Processed row 10163\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, torch_dtype=torch.float16)\n",
    "model = model.to(device='cuda:1')\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\n",
    "\n",
    "# Define the function to process each row\n",
    "def process_row(image_path, extracted_text, prompt):\n",
    "    try:\n",
    "        # Load and convert the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Prepare the input text\n",
    "        input_text = f\"Extracted text: {extracted_text}\\nPrompt: {prompt}\"\n",
    "\n",
    "        msgs = [{'role': 'user', 'content': input_text}]\n",
    "\n",
    "        # Generate response from the model\n",
    "        res = model.chat(\n",
    "            image=image,\n",
    "            msgs=msgs,\n",
    "            tokenizer=tokenizer,\n",
    "            sampling=True,\n",
    "            temperature=0.7,  \n",
    "            stream=False\n",
    "        )\n",
    "\n",
    "\n",
    "        # Collect the generated response\n",
    "        generated_text = \"\"\n",
    "        for new_text in res:\n",
    "            generated_text += new_text\n",
    "\n",
    "        return generated_text.strip()  # Return the final output\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Read the CSV file (input data)\n",
    "input_csv_path = 'dataset/final_test.csv'\n",
    "output_csv_path = 'output10000_15000.csv'\n",
    "test_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Ensure 'easyocr_text' and 'tesseract_text' are strings, replacing NaN with empty strings\n",
    "test_df['easyocr_text'] = test_df['easyocr_text'].fillna('').astype(str)\n",
    "test_df['tesseract_text'] = test_df['tesseract_text'].fillna('').astype(str)\n",
    "\n",
    "# Prepare the prompt for each row\n",
    "prompt_template = \"Identify the {entity_name} from the text and image and output only two words. The unit should be one of {allowed_units}. reply in very short and answer inteligently\"\n",
    "\n",
    "# Initialize the output CSV file\n",
    "output_df = pd.DataFrame(columns=['index', 'prediction'])\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Iterate over each row in the CSV\n",
    "for index, row in test_df.iterrows():\n",
    "    if(index < 10000): continue\n",
    "    image_names = row['image_name']\n",
    "    image_path = \"/raid/ai23mtech11004/amazon-ml/test_images/\" + image_names\n",
    "    extracted_text = row['easyocr_text'] + \" \" + row['tesseract_text']  # Combine texts\n",
    "    entity_name = row['entity_name']\n",
    "    \n",
    "    # Define allowed units based on the entity_name\n",
    "    allowed_units = \", \".join(entity_unit_map.get(entity_name, []))\n",
    "    \n",
    "    # Skip if no allowed units are defined for this entity\n",
    "    if not allowed_units:\n",
    "        results = {\"index\": index, \"prediction\": \"\"}\n",
    "    else:\n",
    "        # Define the prompt based on entity_name\n",
    "        prompt = prompt_template.format(entity_name=entity_name, allowed_units=allowed_units)\n",
    "    \n",
    "        # Process the row to generate the predicted entity_value\n",
    "        entity_value = process_row(image_path, extracted_text, prompt)\n",
    "    \n",
    "        # Append the result\n",
    "        results = {\"index\": index, \"prediction\": entity_value}\n",
    "\n",
    "    # Append the result to the CSV file incrementally\n",
    "    result_df = pd.DataFrame([results])\n",
    "    result_df.to_csv(output_csv_path, mode='a', header=False, index=False)\n",
    "\n",
    "    # Optional: Print status\n",
    "    print(f\"Processed row {index}\")\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
