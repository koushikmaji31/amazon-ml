{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from unit_extractor import extract_value_unit\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "def load_faster_rcnn():\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to extract region proposals from the image\n",
    "def get_region_proposals(model, image_path):\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Get region proposals from the model\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)  # Outputs: boxes, labels, scores\n",
    "\n",
    "    boxes = predictions[0]['boxes']  # Get bounding boxes\n",
    "    return boxes, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load CLIP model\n",
    "def load_clip_model():\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return model, preprocess\n",
    "\n",
    "# Match the region with the entity_name using CLIP\n",
    "def get_best_region(boxes, image, entity_name, clip_model, preprocess):\n",
    "    best_region = None\n",
    "    best_similarity = -1  # Initialize with a low value\n",
    "\n",
    "    # Encode entity_name to text features\n",
    "    text_input = clip.tokenize([entity_name]).to(next(clip_model.parameters()).device)\n",
    "    text_features = clip_model.encode_text(text_input)\n",
    "\n",
    "    # Process each region and calculate similarity\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        region = image.crop((x1, y1, x2, y2))\n",
    "        region = preprocess(region).unsqueeze(0).to(next(clip_model.parameters()).device)\n",
    "\n",
    "        # Compute region features\n",
    "        region_features = clip_model.encode_image(region)\n",
    "\n",
    "        # Compute similarity between region and entity_name\n",
    "        similarity = torch.cosine_similarity(text_features, region_features).item()\n",
    "\n",
    "        # Select the region with the highest similarity\n",
    "        if similarity > best_similarity:\n",
    "            best_region = region\n",
    "            best_similarity = similarity\n",
    "\n",
    "    return best_region, best_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "# Perform OCR on the best region and compare it with the ground truth\n",
    "def ocr_and_loss(best_region, ground_truth):\n",
    "    # Apply OCR on the best region\n",
    "    ocr_result = pytesseract.image_to_string(best_region).strip().lower()\n",
    "\n",
    "    ocr_result = extract_value_unit(ocr_result)\n",
    "\n",
    "    # Ground truth entity value\n",
    "    target = ground_truth.strip().lower()\n",
    "\n",
    "    # Custom cross-entropy loss: 1 if wrong, 0 if correct\n",
    "    return 0 if ocr_result == target else 1\n",
    "\n",
    "# Example of string-based loss function\n",
    "def cross_entropy_loss(pred, target):\n",
    "    return 0 if pred.strip().lower() == target.strip().lower() else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_image_paths, train_entity_names, train_labels, num_epochs=10):\n",
    "    # Load Faster R-CNN and CLIP models\n",
    "    faster_rcnn_model = load_faster_rcnn()\n",
    "    clip_model, preprocess = load_clip_model()\n",
    "\n",
    "    optimizer = torch.optim.Adam(clip_model.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for image_path, entity_name, ground_truth in zip(train_image_paths, train_entity_names, train_labels):\n",
    "            # 1. Get region proposals\n",
    "            boxes, image = get_region_proposals(faster_rcnn_model, image_path)\n",
    "\n",
    "            # 2. Find the best matching region\n",
    "            best_region, best_similarity = get_best_region(boxes, image, entity_name, clip_model, preprocess)\n",
    "\n",
    "            # 3. Apply OCR and calculate loss\n",
    "            if best_region is not None:\n",
    "                loss = ocr_and_loss(best_region, ground_truth)\n",
    "                total_loss += loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA1/ai23mtech12001/miniconda3/envs/amazonml/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/DATA1/ai23mtech12001/miniconda3/envs/amazonml/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /DATA1/ai23mtech12001/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      " 30%|██▉       | 47.1M/160M [00:00<00:02, 50.5MB/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m train_image_paths \u001b[38;5;241m=\u001b[39m train_image_paths\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_entity_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_image_paths, train_entity_names, train_labels, num_epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(train_image_paths, train_entity_names, train_labels, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Load Faster R-CNN and CLIP models\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     faster_rcnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_faster_rcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     clip_model, preprocess \u001b[38;5;241m=\u001b[39m load_clip_model()\n\u001b[1;32m      6\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(clip_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mload_faster_rcnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_faster_rcnn\u001b[39m():\n\u001b[0;32m----> 9\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mfasterrcnn_resnet50_fpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/torchvision/models/detection/faster_rcnn.py:577\u001b[0m, in \u001b[0;36mfasterrcnn_resnet50_fpn\u001b[0;34m(weights, progress, num_classes, weights_backbone, trainable_backbone_layers, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m model \u001b[38;5;241m=\u001b[39m FasterRCNN(backbone, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;241m==\u001b[39m FasterRCNN_ResNet50_FPN_Weights\u001b[38;5;241m.\u001b[39mCOCO_V1:\n\u001b[1;32m    579\u001b[0m         overwrite_eps(model, \u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/torchvision/models/_api.py:90\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/torch/hub.py:765\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    763\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[1;32m    764\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/torch/hub.py:658\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hash_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     sha256\u001b[38;5;241m.\u001b[39mupdate(buffer)  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV file\n",
    "train_df = pd.read_csv(\"/DATA1/ai23mtech12001/Amazon/amazon-ml/dataset/train.csv\")\n",
    "\n",
    "# Extract image paths, entity names, and labels\n",
    "train_image_paths = \"train_images/\" + train_df['image_link'].apply(lambda x: x.split('/')[-1])\n",
    "train_entity_names = train_df['entity_name'].tolist()\n",
    "train_labels = train_df['entity_value'].tolist()\n",
    "train_image_paths = train_image_paths.tolist()\n",
    "\n",
    "# Train the model\n",
    "train_model(train_image_paths, train_entity_names, train_labels, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image_path, entity_name):\n",
    "    # Load pre-trained models\n",
    "    faster_rcnn_model = load_faster_rcnn()\n",
    "    clip_model, preprocess = load_clip_model()\n",
    "\n",
    "    # Get region proposals\n",
    "    boxes, image = get_region_proposals(faster_rcnn_model, image_path)\n",
    "\n",
    "    # Find the best region\n",
    "    best_region, best_similarity = get_best_region(boxes, image, entity_name, clip_model, preprocess)\n",
    "\n",
    "    # Extract text using OCR from the best region\n",
    "    if best_region is not None:\n",
    "        ocr_result = pytesseract.image_to_string(best_region)\n",
    "        print(f\"OCR Result: {ocr_result}\")\n",
    "    else:\n",
    "        print(\"No matching region found.\")\n",
    "\n",
    "# Example inference call\n",
    "inference(\"/DATA1/ai23mtech12001/Amazon/amazon-ml/test_images/21+i52HRW4L.jpg\", \"entity_name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazonml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
