{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 ounce\n",
      "\n",
      "10 gram\n",
      "\n",
      "10 gram\n",
      "\n",
      "11 ounce\n",
      "\n",
      "11 ounce\n",
      "\n",
      "15 kilogram\n",
      "\n",
      "4 fluid ounce\n",
      "\n",
      "7 foot\n",
      "\n",
      "3 cubic inch\n",
      "\n",
      "30 watt\n",
      "\n",
      "15 centimetre\n",
      "\n",
      "20 fluid ounce\n",
      "\n",
      "5000 volt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from unit_extractor import extract_value_unit\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "def load_faster_rcnn():\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to extract region proposals from the image\n",
    "def get_region_proposals(model, image_path):\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Get region proposals from the model\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)  # Outputs: boxes, labels, scores\n",
    "\n",
    "    boxes = predictions[0]['boxes']  # Get bounding boxes\n",
    "    return boxes, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load CLIP model\n",
    "def load_clip_model():\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return model, preprocess\n",
    "\n",
    "# Match the region with the entity_name using CLIP\n",
    "def get_best_region(boxes, image, entity_name, clip_model, preprocess):\n",
    "    best_region = None\n",
    "    best_similarity = -1  # Initialize with a low value\n",
    "\n",
    "    # Encode entity_name to text features\n",
    "    text_input = clip.tokenize([entity_name]).to(next(clip_model.parameters()).device)\n",
    "    text_features = clip_model.encode_text(text_input)\n",
    "\n",
    "    # Process each region and calculate similarity\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        region = image.crop((x1, y1, x2, y2))\n",
    "        region = preprocess(region).unsqueeze(0).to(next(clip_model.parameters()).device)\n",
    "\n",
    "        # Compute region features\n",
    "        region_features = clip_model.encode_image(region)\n",
    "\n",
    "        # Compute similarity between region and entity_name\n",
    "        similarity = torch.cosine_similarity(text_features, region_features).item()\n",
    "\n",
    "        # Select the region with the highest similarity\n",
    "        if similarity > best_similarity:\n",
    "            best_region = region\n",
    "            best_similarity = similarity\n",
    "\n",
    "    return best_region, best_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Set Tesseract command path\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def tensor_to_pil_image(tensor):\n",
    "    # 1. Remove batch dimension\n",
    "    tensor = tensor.squeeze(0)  # From [1, 3, 224, 224] to [3, 224, 224]\n",
    "\n",
    "    # 2. Convert torch tensor to NumPy array\n",
    "    # If the tensor is normalized, you need to un-normalize it\n",
    "    # Assuming the tensor values are in the range [0, 1], convert to [0, 255]\n",
    "    tensor = tensor.mul(255).byte()  # Convert to byte (uint8)\n",
    "\n",
    "    # 3. Permute the channels from [C, H, W] to [H, W, C]\n",
    "    tensor = tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # 4. Convert NumPy array to PIL image\n",
    "    image = Image.fromarray(tensor)\n",
    "    return image\n",
    "\n",
    "def ocr_and_loss(best_region, ground_truth):\n",
    "    # Convert the torch tensor to a PIL image\n",
    "    if isinstance(best_region, torch.Tensor):\n",
    "        best_region = tensor_to_pil_image(best_region)\n",
    "\n",
    "    ocr_result = pytesseract.image_to_string(best_region).strip().lower()\n",
    "\n",
    "    ocr_result = extract_value_unit(ocr_result)\n",
    "\n",
    "    # Ground truth entity value\n",
    "    target = ground_truth.strip().lower()\n",
    "\n",
    "    # Custom cross-entropy loss: 1 if wrong, 0 if correct\n",
    "    return 0 if ocr_result == target else 1\n",
    "\n",
    "# Example of string-based loss function\n",
    "def cross_entropy_loss(pred, target):\n",
    "    return 0 if pred.strip().lower() == target.strip().lower() else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_image_paths, train_entity_names, train_labels, num_epochs=10):\n",
    "    # Load Faster R-CNN and CLIP models\n",
    "    faster_rcnn_model = load_faster_rcnn()\n",
    "    clip_model, preprocess = load_clip_model()\n",
    "\n",
    "    optimizer = torch.optim.Adam(clip_model.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for image_path, entity_name, ground_truth in zip(train_image_paths, train_entity_names, train_labels):\n",
    "            # 1. Get region proposals\n",
    "            boxes, image = get_region_proposals(faster_rcnn_model, image_path)\n",
    "\n",
    "            # 2. Find the best matching region\n",
    "            best_region, best_similarity = get_best_region(boxes, image, entity_name, clip_model, preprocess)\n",
    "\n",
    "            # 3. Apply OCR and calculate loss\n",
    "            if best_region is not None:\n",
    "                loss = ocr_and_loss(best_region, ground_truth)\n",
    "                total_loss += loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA1/ai23mtech12001/miniconda3/envs/amazonml/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/DATA1/ai23mtech12001/miniconda3/envs/amazonml/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TesseractNotFoundError",
     "evalue": "/usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/pytesseract/pytesseract.py:275\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msubprocess_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/subprocess.py:1837\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1836\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/bin/tesseract'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m train_image_paths \u001b[38;5;241m=\u001b[39m train_image_paths\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_entity_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_image_paths, train_entity_names, train_labels, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# 3. Apply OCR and calculate loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_region \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mocr_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_region\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 29\u001b[0m, in \u001b[0;36mocr_and_loss\u001b[0;34m(best_region, ground_truth)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(best_region, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     27\u001b[0m     best_region \u001b[38;5;241m=\u001b[39m tensor_to_pil_image(best_region)\n\u001b[0;32m---> 29\u001b[0m ocr_result \u001b[38;5;241m=\u001b[39m \u001b[43mpytesseract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_region\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     31\u001b[0m ocr_result \u001b[38;5;241m=\u001b[39m extract_value_unit(ocr_result)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Ground truth entity value\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/pytesseract/pytesseract.py:486\u001b[0m, in \u001b[0;36mimage_to_string\u001b[0;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBYTES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDICT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRING\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/pytesseract/pytesseract.py:489\u001b[0m, in \u001b[0;36mimage_to_string.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    487\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[1;32m    488\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[0;32m--> 489\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    490\u001b[0m }[output_type]()\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/pytesseract/pytesseract.py:352\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[1;32m    342\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    350\u001b[0m     }\n\u001b[0;32m--> 352\u001b[0m     \u001b[43mrun_tesseract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_output(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    355\u001b[0m         return_bytes,\n\u001b[1;32m    356\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/amazonml/lib/python3.9/site-packages/pytesseract/pytesseract.py:280\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m: /usr/bin/tesseract is not installed or it's not in your PATH. See README file for more information."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV file\n",
    "train_df = pd.read_csv(\"/DATA1/ai23mtech12001/Amazon/amazon-ml/dataset/train.csv\")\n",
    "\n",
    "# Extract image paths, entity names, and labels\n",
    "train_image_paths = \"train_images/\" + train_df['image_link'].apply(lambda x: x.split('/')[-1])\n",
    "train_entity_names = train_df['entity_name'].tolist()\n",
    "train_labels = train_df['entity_value'].tolist()\n",
    "train_image_paths = train_image_paths.tolist()\n",
    "\n",
    "# Train the model\n",
    "train_model(train_image_paths, train_entity_names, train_labels, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image_path, entity_name):\n",
    "    # Load pre-trained models\n",
    "    faster_rcnn_model = load_faster_rcnn()\n",
    "    clip_model, preprocess = load_clip_model()\n",
    "\n",
    "    # Get region proposals\n",
    "    boxes, image = get_region_proposals(faster_rcnn_model, image_path)\n",
    "\n",
    "    # Find the best region\n",
    "    best_region, best_similarity = get_best_region(boxes, image, entity_name, clip_model, preprocess)\n",
    "\n",
    "    # Extract text using OCR from the best region\n",
    "    if best_region is not None:\n",
    "        ocr_result = pytesseract.image_to_string(best_region)\n",
    "        print(f\"OCR Result: {ocr_result}\")\n",
    "    else:\n",
    "        print(\"No matching region found.\")\n",
    "\n",
    "# Example inference call\n",
    "inference(\"/DATA1/ai23mtech12001/Amazon/amazon-ml/test_images/21+i52HRW4L.jpg\", \"entity_name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazonml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
